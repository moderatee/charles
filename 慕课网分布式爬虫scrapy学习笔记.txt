快捷键设置：
vim ~/.bashrc
==alias pycharm='/home/charles/下载/pycharm-2018.2.4/bin/pycharm.sh'==
source ~/.bashrc

解压安装包 tar -zxvf *.tar.gz(z是针对gz格式，x是解压)

磁盘空间不够(修改成200G,不能往回缩了)
1. 虚拟机设置
sudo fdisk -l
fdisk /dev/sda  编辑
n 增加新分区
p 创建主分区
[root@localhost ~]# mkfs  /dev/sda3
mke2fs 1.42.9 (28-Dec-2013)
无法对 /dev/sda3 进行 stat 调用 --- 没有那个文件或目录

The device apparently does not exist; did you specify it correctly?

解决方法：执行下partprobe 命令

[root@localhost ~]# partprobe
[root@localhost ~]# mkfs -t ext4 /dev/sda3

mount /dev/sda3 /exapp
挂载
vi /etc/fstab
/dev/sda3           /root/exapp           ext3         defaults         1 2


linux装mysql
Ubuntu用apt-get命令
centos用yum安装

https://dev.mysql.com/downloads/file/?id=465603   5.7版本的yum
[root@localhost Software]# yum install -y mysql57-community-release-el7-9.noarch.rpm
检查安装
[root@localhost Software]# yum repolist all | grep mysql
[root@localhost Software]# yum install -y  mysql-community-server


虚拟机安装报错
空间需求：
 570 MB 以上的空间至少需要在文件系统 / 上。   ----未解决


-------------------------------------------
 修改bind-address
 修改数据库root权限
 navicat设置utf8，general_ci

安装virtualenv
 pip install virtualenv


 豆瓣源
 https://pypi.douban.com/simple/
 pip  install  -i  https://pypi.doubanio.com/simple/  --trusted-host pypi.doubanio.com  django

virtualenv scpapytest
C:\Users\User\scpapytest\Scripts>activate.bat

virtualenv -p "D:\Program Files\Python\Python27\python.exe" scrapy27

pip install virtualenvwrapper

workon
C:\Users\User\scpapytest\Scripts\scrapy27\Scripts>mkvirtualenv.bat --python="D:\Program Files\Python\Python27\python.exe" py27

修改环境变量需要重启cmd


关于Scrapy安装：Microsoft visual c++ 14.0 is required的解决方法总结
Twisted-18.9.0-cp36-cp36m-win_amd64.whl

mkvirtualenv.bat --python="D:\Program Files\Python\Python36\python.exe" py36


对于virtualenv和virtualenvwrapper终于会用了。
pip 安装的是属于系统环境变量里面python的
virtualenv和mkvirtualenv的区别

virtualenvwrapper-win安装会让你能够使用workon，在任何路径下

pip list可以查看当前env环境下的python的环境

卸载mysql，这视频默认安装也是够了。。。占用偶的C盘环境
安装要选择custom，自定义，点击server，会有advanced options选项，选择路径。点击msi后，出现server8.0,原因是我的community没有卸载。


virtualenv和mkvirtualenv的区别
virtualenv是在当前目录下创建虚拟环境（手动删除）
mkvirtualenv会在WORK_HMOE下创建虚拟环境（可以rmvirtualenv删除）

requests和beautifulsoup都是库，scrapy是库，scrapy基于twisted


import re

line = 'booooooooobby123'
regex_str = ".*(b.*b).*"       #bb
regex_str1 = ".*?(b.*b).*"		#booooobb
regex_str2 = ".*?(b.*?b).*"     #booooob

match_obj=re.match(regex_str, line)
if match_obj:
    print(match_obj.group(1))

line = 'boobby123'
regex_str = {(bobby|booby)123}      #bb
match_obj=re.match(regex_str, line)
if match_obj:
    print(match_obj.group(1))

--------------------------------
import re
line = 'boobby123'
regex_str = "((bobby|boobby)123)"      
match_obj=re.match(regex_str, line)
if match_obj:
	print(match_obj.group(2))     #boobby
	print(match_obj.group(1)) 	#boobby123

出现在[]中不具有特殊含义. *

\s空格  \S非空格 space
\w英文字母，数字，下划线  word 
[\u4E00-\u9FA5]  汉字

import re
#line = 'boobby123'
line="study in 南京大学"
#regex_str = ".*([\u4E00-\u9FA5]+大学)"      #京大学
regex_str = ".*？([\u4E00-\u9FA5]+大学)"	#南京大学
match_obj=re.match(regex_str, line)
if match_obj:
	print(match_obj.group(1))


line="xxx出生于2001年"
regex_str=".*?(\d+)年"   #不加?会出现1，加了？会匹配2001

出生于2001年6月1日
出生于2001/6/1
出生于2001-6-1
出生于2001-06-01
出生于2001-06
regex_str=".*出生于(\d{4}[年/-]\d{1,2}([月/-]\d{1,2}|[月/-]$|$))"

unicode解决乱码，但用的字节过多，ASCLL英文，只有65位

用python2.7编码
>>> s="我用python"
>>> su=u"我用python"#Unicode编码
>>> s.encode("utf-8")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
UnicodeDecodeError: 'ascii' codec can't decode byte 0xce in position 0: ordinal
not in range(128)

>>> s.decode("gb2312").encode("utf-8")#用gb2312解码成unicode，再显示成utf-8
'\xe6\x88\x91\xe7\x94\xa8python'

windows系统用gb2312解码，linnux用utf-8解码decode

>>> sys.getdefaultencoding()
'ascii'
>>> su.encode('utf-8')
'\xe6\x88\x91\xe7\x94\xa8python'

python3是用unicode编码
>>> s="我用python"
>>> s.encode('utf-8')
b'\xe6\x88\x91\xe7\x94\xa8python'


网址去重
深度优先（递归实现---父子，参数）
先爬取一个分支到根节点，爬完返回到分支，再爬另一个分支
广度优先（宽度优先，队列实现）
先访问第一层，再访问第二层，再访问第三次
将a放进去，第一次，a弹出来，将a的子节点b，c放进去，
第一次，b.c弹出来，将b,c的子节点e,f,g,h放进去。。。


爬虫去重策略
保存到set中
url经过md5保存到set
用bitmap，将url通过hash函数映射到某一位
(解决冲突，冲突的可能性十分大)

bloomfilter改进bitmap，多重hash解决冲突
1亿/8(8个byte)/1024(KB)/1024=12MB


scrapy startproject ArticleSpider

在ArticleSpider的目录下执行
scrapy genspider jobboie blog.jobboie.com


scrapy crawl jobbole
No module named 'win32api'
pip  install  -i  https://pypi.doubanio.com/simple/  --trusted-host pypi.doubanio.com  pypiwin32

settings设置
ROBOTSTXT_OBEY = False

xpath语法
article：选取article元素所有子节点
/article:选取根元素article
article：选取所有属于article的子元素的a元素
//div   :选取所有的dic子元素
article//div:选取所有属于article元素的后代的div元素
//@class  选取所有名为class的属性

/article/div[1]
/article/div[last()]
//div[@lang]  选取所有拥有lang属性的div
//div[@lang='eng']

/div/*
//div[@*]

scrapy shell url
title=response.xpath()
title.extract()[0]

selector=response.xpath('//*[@id="post-114466"]/div[1]/h1/text()')
//*[@id="post-114466"]/div[3]/div[7]/span[1]
收藏数//span[contains(@class,'bookmark-btn')]/h10/text()
点赞数response.xpath("//span[contains(@class, 'vote-post-up')]/h10/text()").extract()[0]
#list过滤
tag_list = [element for element in tag_list if not element.strip().endswith("评论")] 
#用，连接
tags = ",".join(tag_list)

css选择器
#container id=container
.class     属性包含class的
li a    所有li包含的所有a节点
ul+p    ul后面第一个p元素
div#container>ul  id为container的div的第一个ul子元素
ul~p   与ul相邻的p元素
a[title]  所有有title的a元素
a[href*='jobble'] 所有href属性包含jobble的a元素
a[href*='jobble']             以jobble开头

li:nth-child(3)   第三个li元素
tr:nth-child(2n)  第偶数个tr
div:not(#container) 所有id非container的div元素

response.css('.entry-header h1::text').extract()

贪婪模式？？？

extract_first()

    def parse(self, response):
        """
        1. 获取文章列表页中的文章url并交给scrapy下载后并进行解析
        2. 获取下一页的url并交给scrapy进行下载， 下载完成后交给parse
        """

        #解析列表页中的所有文章url并交给scrapy下载后并进行解析
        post_nodes = response.css("#archive .floated-thumb .post-thumb a")
        for post_node in post_nodes:
        #嵌套使用css方法
            image_url = post_node.css("img::attr(src)").extract_first("")
            post_url = post_node.css("::attr(href)").extract_first("")
            #url这是是等于post_url,是为了防止post_url是相对地址，yield是scrapy内置的的方法
            #callback是回调函数，parse_detail解析主页面里的单个url里面的字段，如文章点赞，标题
            yield Request(url=parse.urljoin(response.url, post_url), meta={"front_image_url":image_url}, callback=self.parse_detail)

        #提取下一页并交给scrapy进行下载
        next_url = response.css(".next.page-numbers::attr(href)").extract_first("")
        if next_url:
            yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse)

		获取在response.meta里面的属性
        front_image_url = response.meta.get("front_image_url", "") 

在items定义JobBoleArticleItem类class JobBoleArticleItem(scrapy.Item)，属性为title = scrapy.Field()，初始化article_item["title"] = title，赋值结束后yield article_item


修改settings里面的ITEM_PIPELINES，去掉注释
'scrapy.pipelines.images.ImagesPipeline': 1   路径：优先处理顺序
IMAGES_URLS_FIELD = "front_image_url"   配置items的字段
project_dir = os.path.abspath(os.path.dirname(__file__))
IMAGES_STORE = os.path.join(project_dir, 'images')   配置储存的位置

图片储存
pip  install  -i  https://pypi.doubanio.com/simple/  --trusted-host pypi.doubanio.com  pillow


图片已经能保存了，但是没有把图片和image_url绑定，需要自定义class ArticleImagePipeline(ImagesPipeline)

设置下载的图片的宽度
# IMAGES_MIN_HEIGHT = 100
# IMAGES_MIN_WIDTH = 100


要删去原来的图片，才能再下载
'ArticleSpider.pipelines.ArticleImagePipeline': 2


import hashlib
import re
def get_md5(url):
    if isinstance(url, str):#str就是一个unicode
        url = url.encode("utf-8")
    m = hashlib.md5()
    m.update(url)
    return m.hexdigest()

    md5加密url

if __name__="__main__"
	print(get_md5("..."))


class JsonExporterPipleline(object):
    #调用scrapy提供的json export导出json文件
    def __init__(self):
        self.file = open('articleexport.json', 'wb')
        self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False)
        self.exporter.start_exporting()

    def close_spider(self, spider):
        self.exporter.finish_exporting()
        self.file.close()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item

import codecs
import json
self.file=codecs.open('article','w',encoding='utf-8')
lines=json.dump(dict(item),ensure_ascll=False)+"\n"
self.file=codecs.open('article','w',encoding='utf-8')


from scrapy.exporters import JsonItemExporter
class JsonExporterPipleline(object):
    #调用scrapy提供的json export导出json文件
    def __init__(self):
        self.file = open('articleexport.json', 'wb')
        self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False)
        self.exporter.start_exporting()

    def close_spider(self, spider):
        self.exporter.finish_exporting()
        self.file.close()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item

pip  install  -i  https://pypi.doubanio.com/simple/  --trusted-host pypi.doubanio.com  mysqlclient


class MysqlPipeline(object):
    #采用同步的机制写入mysql
    def __init__(self):
        self.conn = MySQLdb.connect(' 172.16.46.66', 'root', 'root', 'article_spider', charset="utf8", use_unicode=True)
        self.cursor = self.conn.cursor()

    def process_item(self, item, spider):
        insert_sql = """
            insert into jobbole_article(title, url, create_date, fav_nums)
            VALUES (%s, %s, %s, %s)
        """
        self.cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"]))
        self.conn.commit()


        from twisted.enterprise import adbapi
        dbpool = adbapi.ConnectionPool("MySQLdb", **dbparms)

        from scrapy.loader import ItemLoader

        item_loader = ArticleItemLoader(item=JobBoleArticleItem(), response=response)
        item_loader.add_css("title", ".entry-header h1::text")
        item_loader.add_value("url", response.url)
        article_item = item_loader.load_item()

		from scrapy.loader.processors import MapCompose, TakeFirst, Join
         create_date = scrapy.Field(
        input_processor=MapCompose(date_convert),
    	)
    	
class ArticleItemLoader(ItemLoader):
    #自定义itemloader
    default_output_processor = TakeFirst()


import requests
try:
    import cookielib
except:
    import http.cookiejar as cookielib
兼容py2和py3